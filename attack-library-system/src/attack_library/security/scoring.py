"""Confidence scoring system for vulnerability assessments."""

import logging
import math
import statistics
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

from .config import SeverityLevel
from .models import DetectionResult, VulnerabilityFinding, VulnerabilityType

logger = logging.getLogger(__name__)


@dataclass
class ScoreExplanation:
    """Explanation for a confidence score calculation."""

    base_score: float
    adjustments: List[Dict[str, Any]]
    final_score: float
    reasoning: List[str]
    confidence_level: str


class ConfidenceScorer:
    """
    Sophisticated confidence scoring mechanism for vulnerability assessments.

    Provides calibrated scores with explanations, multi-factor scoring,
    and score aggregation for complex vulnerability scenarios.
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """Initialize confidence scorer with configuration."""
        self.config = config or {}

        # Scoring weights for different factors
        self.scoring_weights = self.config.get(
            "scoring_weights",
            {
                "detection_consistency": 0.3,  # How consistent are multiple detections
                "evidence_strength": 0.25,  # Quality and quantity of evidence
                "strategy_reliability": 0.2,  # Reliability of detection strategies
                "pattern_specificity": 0.15,  # How specific are the matched patterns
                "contextual_relevance": 0.1,  # Relevance to attack context
            },
        )

        # Strategy reliability scores (based on empirical performance)
        self.strategy_reliability = self.config.get(
            "strategy_reliability",
            {"rule_based": 0.7, "ml_based": 0.85, "heuristic": 0.6, "combined": 0.9},
        )

        # Calibration parameters for score adjustment
        self.calibration_params = self.config.get(
            "calibration_params",
            {
                "confidence_threshold": 0.5,
                "overconfidence_penalty": 0.1,
                "underconfidence_boost": 0.05,
            },
        )

        # Historical data for calibration (would be loaded from file in production)
        self.historical_performance = {}

        logger.info("ConfidenceScorer initialized with weighted scoring")

    def calculate_confidence_score(
        self,
        detection_results: List[DetectionResult],
        vulnerability_type: VulnerabilityType,
        context: Optional[Dict[str, Any]] = None,
    ) -> Tuple[float, ScoreExplanation]:
        """
        Calculate calibrated confidence score for a vulnerability.

        Args:
            detection_results: Results from detection strategies
            vulnerability_type: Type of vulnerability being scored
            context: Additional context for scoring

        Returns:
            Tuple of (confidence_score, explanation)
        """
        if not detection_results:
            return 0.0, ScoreExplanation(
                base_score=0.0,
                adjustments=[],
                final_score=0.0,
                reasoning=["No detection results provided"],
                confidence_level="none",
            )

        context = context or {}
        adjustments = []
        reasoning = []

        # 1. Calculate base score from detection results
        base_score = self._calculate_base_score(detection_results)
        reasoning.append(f"Base score from {len(detection_results)} detection(s): {base_score:.3f}")

        # 2. Adjust for detection consistency
        consistency_adj = self._calculate_consistency_adjustment(detection_results)
        adjustments.append(
            {
                "type": "consistency",
                "adjustment": consistency_adj,
                "weight": self.scoring_weights["detection_consistency"],
            }
        )
        if abs(consistency_adj) > 0.01:
            reasoning.append(f"Consistency adjustment: {consistency_adj:+.3f}")

        # 3. Adjust for evidence strength
        evidence_adj = self._calculate_evidence_adjustment(detection_results)
        adjustments.append(
            {
                "type": "evidence_strength",
                "adjustment": evidence_adj,
                "weight": self.scoring_weights["evidence_strength"],
            }
        )
        if abs(evidence_adj) > 0.01:
            reasoning.append(f"Evidence strength adjustment: {evidence_adj:+.3f}")

        # 4. Adjust for strategy reliability
        reliability_adj = self._calculate_reliability_adjustment(detection_results)
        adjustments.append(
            {
                "type": "strategy_reliability",
                "adjustment": reliability_adj,
                "weight": self.scoring_weights["strategy_reliability"],
            }
        )
        if abs(reliability_adj) > 0.01:
            reasoning.append(f"Strategy reliability adjustment: {reliability_adj:+.3f}")

        # 5. Adjust for pattern specificity
        specificity_adj = self._calculate_specificity_adjustment(detection_results)
        adjustments.append(
            {
                "type": "pattern_specificity",
                "adjustment": specificity_adj,
                "weight": self.scoring_weights["pattern_specificity"],
            }
        )
        if abs(specificity_adj) > 0.01:
            reasoning.append(f"Pattern specificity adjustment: {specificity_adj:+.3f}")

        # 6. Adjust for contextual relevance
        context_adj = self._calculate_contextual_adjustment(detection_results, context)
        adjustments.append(
            {
                "type": "contextual_relevance",
                "adjustment": context_adj,
                "weight": self.scoring_weights["contextual_relevance"],
            }
        )
        if abs(context_adj) > 0.01:
            reasoning.append(f"Contextual relevance adjustment: {context_adj:+.3f}")

        # Calculate weighted final score
        weighted_adjustments = sum(adj["adjustment"] * adj["weight"] for adj in adjustments)

        preliminary_score = base_score + weighted_adjustments

        # Apply calibration
        final_score = self._apply_calibration(preliminary_score, vulnerability_type)

        if abs(final_score - preliminary_score) > 0.01:
            reasoning.append(f"Calibration adjustment: {final_score - preliminary_score:+.3f}")

        # Ensure score is in valid range
        final_score = max(0.0, min(1.0, final_score))

        # Determine confidence level
        confidence_level = self._determine_confidence_level(final_score)

        explanation = ScoreExplanation(
            base_score=base_score,
            adjustments=adjustments,
            final_score=final_score,
            reasoning=reasoning,
            confidence_level=confidence_level,
        )

        return final_score, explanation

    def _calculate_base_score(self, detection_results: List[DetectionResult]) -> float:
        """Calculate base confidence score from detection results."""
        if not detection_results:
            return 0.0

        # Weight individual scores by strategy reliability
        weighted_scores = []

        for result in detection_results:
            strategy_weight = self.strategy_reliability.get(result.strategy_name, 0.5)
            weighted_score = result.confidence_score * strategy_weight
            weighted_scores.append(weighted_score)

        # Use weighted average as base score
        base_score = sum(weighted_scores) / len(weighted_scores)

        return base_score

    def _calculate_consistency_adjustment(self, detection_results: List[DetectionResult]) -> float:
        """Calculate adjustment based on consistency across detections."""
        if len(detection_results) <= 1:
            return 0.0

        scores = [r.confidence_score for r in detection_results]

        # Calculate standard deviation of scores
        std_dev = statistics.stdev(scores) if len(scores) > 1 else 0.0

        # Lower standard deviation = higher consistency = positive adjustment
        # Normalize std_dev to [0,1] range and convert to adjustment
        consistency_score = max(0.0, 1.0 - (std_dev * 2))  # std_dev of 0.5 = no adjustment

        # Scale to adjustment range [-0.2, +0.2]
        adjustment = (consistency_score - 0.5) * 0.4

        return adjustment

    def _calculate_evidence_adjustment(self, detection_results: List[DetectionResult]) -> float:
        """Calculate adjustment based on strength and quality of evidence."""
        if not detection_results:
            return 0.0

        total_evidence_items = sum(len(r.evidence) for r in detection_results)

        # More evidence generally increases confidence
        if total_evidence_items >= 5:
            evidence_adjustment = 0.1
        elif total_evidence_items >= 3:
            evidence_adjustment = 0.05
        elif total_evidence_items >= 1:
            evidence_adjustment = 0.0
        else:
            evidence_adjustment = -0.1

        # Quality assessment based on evidence specificity
        specific_evidence = 0
        for result in detection_results:
            for evidence in result.evidence:
                if len(evidence) > 50:  # Detailed evidence
                    specific_evidence += 1
                elif len(evidence) > 20:  # Medium detail
                    specific_evidence += 0.5

        quality_adjustment = min(0.1, specific_evidence * 0.02)

        return evidence_adjustment + quality_adjustment

    def _calculate_reliability_adjustment(self, detection_results: List[DetectionResult]) -> float:
        """Calculate adjustment based on strategy reliability scores."""
        if not detection_results:
            return 0.0

        # Get average reliability of strategies used
        reliabilities = [
            self.strategy_reliability.get(r.strategy_name, 0.5) for r in detection_results
        ]

        avg_reliability = sum(reliabilities) / len(reliabilities)

        # Convert reliability to adjustment: reliability of 0.7 = no adjustment
        baseline_reliability = 0.7
        adjustment = (avg_reliability - baseline_reliability) * 0.3

        return adjustment

    def _calculate_specificity_adjustment(self, detection_results: List[DetectionResult]) -> float:
        """Calculate adjustment based on pattern/detection specificity."""
        if not detection_results:
            return 0.0

        specificity_scores = []

        for result in detection_results:
            # Analyze metadata for specificity indicators
            metadata = result.metadata
            specificity = 0.5  # Default

            # Check for specific patterns or matches
            if "pattern" in metadata:
                specificity += 0.2
            if "exact_match" in metadata:
                specificity += 0.3
            if "confidence_threshold" in metadata:
                specificity += 0.1

            specificity_scores.append(min(1.0, specificity))

        avg_specificity = sum(specificity_scores) / len(specificity_scores)

        # Convert to adjustment: specificity of 0.5 = no adjustment
        adjustment = (avg_specificity - 0.5) * 0.2

        return adjustment

    def _calculate_contextual_adjustment(
        self, detection_results: List[DetectionResult], context: Dict[str, Any]
    ) -> float:
        """Calculate adjustment based on contextual relevance."""
        if not context:
            return 0.0

        adjustment = 0.0

        # Check for attack prompt context
        if "attack_prompt" in context:
            # Bonus for detections that relate to the specific attack
            adjustment += 0.05

        # Check for model-specific context
        if "model_name" in context:
            # Some vulnerabilities are more/less likely for certain models
            # This would be based on historical data
            adjustment += 0.02

        # Check for response characteristics
        if "response_length" in context:
            response_length = context["response_length"]
            # Very short or very long responses might be more suspicious
            if response_length < 50 or response_length > 2000:
                adjustment += 0.03

        return min(0.1, adjustment)  # Cap adjustment

    def _apply_calibration(self, score: float, vulnerability_type: VulnerabilityType) -> float:
        """Apply calibration based on historical performance data."""
        # This would use historical data to adjust scores based on
        # actual performance vs predicted scores

        # For now, apply simple calibration rules
        calibrated_score = score

        # Reduce overconfidence for high scores
        if score > 0.8:
            overconfidence_penalty = (score - 0.8) * self.calibration_params[
                "overconfidence_penalty"
            ]
            calibrated_score -= overconfidence_penalty

        # Slight boost for moderate scores that tend to be underconfident
        elif 0.3 <= score <= 0.6:
            calibrated_score += self.calibration_params["underconfidence_boost"]

        return calibrated_score

    def _determine_confidence_level(self, score: float) -> str:
        """Determine descriptive confidence level from score."""
        if score >= 0.9:
            return "very_high"
        elif score >= 0.7:
            return "high"
        elif score >= 0.5:
            return "medium"
        elif score >= 0.3:
            return "low"
        else:
            return "very_low"

    def aggregate_vulnerability_scores(
        self, vulnerabilities: List[VulnerabilityFinding], aggregation_method: str = "weighted_max"
    ) -> Tuple[float, Dict[str, Any]]:
        """
        Aggregate multiple vulnerability scores into overall risk score.

        Args:
            vulnerabilities: List of vulnerability findings
            aggregation_method: Method for aggregation ('weighted_max', 'average', 'severity_weighted')

        Returns:
            Tuple of (overall_score, aggregation_metadata)
        """
        if not vulnerabilities:
            return 0.0, {"method": aggregation_method, "count": 0}

        scores = [v.confidence_score for v in vulnerabilities]
        severities = [v.severity for v in vulnerabilities]

        if aggregation_method == "weighted_max":
            # Take max score but weight by number of findings
            max_score = max(scores)
            count_multiplier = min(1.2, 1.0 + (len(vulnerabilities) - 1) * 0.05)
            overall_score = min(1.0, max_score * count_multiplier)

        elif aggregation_method == "average":
            overall_score = sum(scores) / len(scores)

        elif aggregation_method == "severity_weighted":
            # Weight scores by severity level
            severity_weights = {
                SeverityLevel.CRITICAL: 1.0,
                SeverityLevel.HIGH: 0.8,
                SeverityLevel.MEDIUM: 0.6,
                SeverityLevel.LOW: 0.4,
                SeverityLevel.INFO: 0.2,
            }

            weighted_sum = sum(
                score * severity_weights.get(severity, 0.5)
                for score, severity in zip(scores, severities)
            )
            total_weight = sum(severity_weights.get(sev, 0.5) for sev in severities)
            overall_score = weighted_sum / max(total_weight, 1.0)

        else:
            raise ValueError(f"Unknown aggregation method: {aggregation_method}")

        metadata = {
            "method": aggregation_method,
            "count": len(vulnerabilities),
            "individual_scores": scores,
            "max_individual": max(scores) if scores else 0.0,
            "min_individual": min(scores) if scores else 0.0,
            "score_variance": statistics.variance(scores) if len(scores) > 1 else 0.0,
        }

        return overall_score, metadata

    def explain_score_calculation(
        self, explanation: ScoreExplanation, detailed: bool = False
    ) -> str:
        """
        Generate human-readable explanation of score calculation.

        Args:
            explanation: Score explanation object
            detailed: Whether to include detailed breakdown

        Returns:
            Formatted explanation string
        """
        lines = []

        lines.append(
            f"Confidence Score: {explanation.final_score:.3f} ({explanation.confidence_level})"
        )
        lines.append("")

        if detailed:
            lines.append(f"Base Score: {explanation.base_score:.3f}")

            if explanation.adjustments:
                lines.append("Adjustments:")
                for adj in explanation.adjustments:
                    if abs(adj["adjustment"]) > 0.001:
                        lines.append(
                            f"  - {adj['type']}: {adj['adjustment']:+.3f} (weight: {adj['weight']:.2f})"
                        )

            lines.append("")

        if explanation.reasoning:
            lines.append("Reasoning:")
            for reason in explanation.reasoning:
                lines.append(f"  â€¢ {reason}")

        return "\n".join(lines)

    def update_calibration_data(
        self,
        predictions: List[float],
        actual_outcomes: List[bool],
        vulnerability_type: VulnerabilityType,
    ):
        """Update calibration data with new prediction vs outcome pairs."""
        # This would update historical performance data
        # For now, just log the update
        logger.info(
            f"Updated calibration data for {vulnerability_type.value}: "
            f"{len(predictions)} new data points"
        )

        # In production, this would:
        # 1. Store prediction-outcome pairs
        # 2. Recalculate calibration curves
        # 3. Update calibration parameters
