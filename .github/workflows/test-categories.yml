name: Categorized Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC for comprehensive tests
  workflow_dispatch:
    inputs:
      test_category:
        description: 'Test category to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - benchmarks

env:
  PYTHON_VERSION: "3.11"

jobs:
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.test_category == 'unit' || github.event.inputs.test_category == 'all' || github.event.inputs.test_category == ''

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run unit tests
      run: |
        pytest tests/unit/ -m unit \
          --cov=src \
          --cov-report=xml \
          --cov-report=term \
          --junitxml=junit-unit.xml \
          -v

    - name: Upload coverage
      uses: codecov/codecov-action@v4
      with:
        token: ${{ secrets.CODECOV_TOKEN }}
        file: ./coverage.xml
        flags: unit
        name: unit-tests

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: unit-test-results
        path: |
          junit-unit.xml
          coverage.xml

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    if: |
      (github.event.inputs.test_category == 'integration' ||
       github.event.inputs.test_category == 'all' ||
       github.event_name == 'schedule') &&
      (github.event_name != 'pull_request' || contains(github.event.pull_request.labels.*.name, 'run-integration'))

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run integration tests (mocked)
      if: github.event_name == 'pull_request'
      run: |
        pytest tests/integration/ -m "integration and not requires_api" \
          --cov=src \
          --cov-report=xml \
          --junitxml=junit-integration.xml \
          -v

    - name: Run integration tests (with APIs)
      if: github.event_name != 'pull_request'
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        pytest tests/integration/ -m integration \
          --cov=src \
          --cov-report=xml \
          --junitxml=junit-integration.xml \
          -v --tb=short

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: integration-test-results
        path: |
          junit-integration.xml
          coverage.xml

  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.test_category == 'e2e' ||
      github.event.inputs.test_category == 'all' ||
      github.event_name == 'schedule'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install -r requirements-test.txt

    - name: Run E2E tests
      run: |
        pytest tests/e2e/ -m e2e \
          --cov=src \
          --cov-report=xml \
          --junitxml=junit-e2e.xml \
          -v --tb=short \
          --timeout=120

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: e2e-test-results
        path: |
          junit-e2e.xml
          coverage.xml

  benchmark-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: |
      github.event.inputs.test_category == 'benchmarks' ||
      github.event.inputs.test_category == 'all' ||
      github.event_name == 'schedule' ||
      github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Need history for comparisons

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install psutil memory-profiler

    - name: Download previous benchmark results
      uses: actions/download-artifact@v4
      with:
        name: benchmark-baseline
        path: ./previous-benchmarks/
      continue-on-error: true

    - name: Run benchmark tests
      run: |
        pytest tests/benchmarks/ -m benchmark \
          --benchmark-save=benchmark-results.json \
          --junitxml=junit-benchmarks.xml \
          -v --tb=short

    - name: Compare with baseline
      if: github.event_name == 'pull_request'
      run: |
        if [ -f "./previous-benchmarks/benchmark-baseline.json" ]; then
          python -c "
          import json
          import sys

          with open('benchmark-results.json') as f:
              current = json.load(f)
          with open('./previous-benchmarks/benchmark-baseline.json') as f:
              baseline = json.load(f)

          # Simple comparison logic
          regressions = []
          for test in current:
              baseline_test = next((b for b in baseline if b['test'] == test['test']), None)
              if baseline_test:
                  current_mean = test['summary']['mean']
                  baseline_mean = baseline_test['summary']['mean']
                  if current_mean > baseline_mean * 1.2:  # 20% regression threshold
                      regressions.append({
                          'test': test['test'],
                          'current': current_mean,
                          'baseline': baseline_mean,
                          'regression': (current_mean / baseline_mean - 1) * 100
                      })

          if regressions:
              print('⚠️ Performance Regressions Detected:')
              for r in regressions:
                  print(f\"  - {r['test']}: {r['regression']:.1f}% slower\")
              sys.exit(1)
          else:
              print('✅ No performance regressions detected')
          "
        else
          echo "No baseline found, skipping comparison"
        fi

    - name: Save benchmark baseline
      if: github.ref == 'refs/heads/main'
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-baseline
        path: benchmark-results.json
        retention-days: 30

    - name: Upload benchmark results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmark-results.json
          junit-benchmarks.xml

  test-summary:
    name: Test Summary
    needs: [unit-tests, integration-tests, e2e-tests, benchmark-tests]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: Download all test results
      uses: actions/download-artifact@v4
      with:
        path: test-results/

    - name: Generate test summary
      run: |
        echo "# Test Results Summary 📊" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "## Test Categories" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        echo "| Category | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|----------|--------|" >> $GITHUB_STEP_SUMMARY

        # Check job results
        if [[ "${{ needs.unit-tests.result }}" == "success" ]]; then
          echo "| Unit Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.unit-tests.result }}" == "skipped" ]]; then
          echo "| Unit Tests | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Unit Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
          echo "| Integration Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.integration-tests.result }}" == "skipped" ]]; then
          echo "| Integration Tests | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Integration Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        if [[ "${{ needs.e2e-tests.result }}" == "success" ]]; then
          echo "| E2E Tests | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.e2e-tests.result }}" == "skipped" ]]; then
          echo "| E2E Tests | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| E2E Tests | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        if [[ "${{ needs.benchmark-tests.result }}" == "success" ]]; then
          echo "| Benchmarks | ✅ Passed |" >> $GITHUB_STEP_SUMMARY
        elif [[ "${{ needs.benchmark-tests.result }}" == "skipped" ]]; then
          echo "| Benchmarks | ⏭️ Skipped |" >> $GITHUB_STEP_SUMMARY
        else
          echo "| Benchmarks | ❌ Failed |" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY

        # Parse test results if available
        if ls test-results/*/junit-*.xml 1> /dev/null 2>&1; then
          echo "## Test Statistics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import xml.etree.ElementTree as ET
          import glob

          total_tests = 0
          total_failures = 0
          total_errors = 0
          total_skipped = 0
          total_time = 0

          for file in glob.glob('test-results/*/junit-*.xml'):
              try:
                  tree = ET.parse(file)
                  root = tree.getroot()

                  if root.tag == 'testsuites':
                      for testsuite in root.findall('testsuite'):
                          total_tests += int(testsuite.get('tests', 0))
                          total_failures += int(testsuite.get('failures', 0))
                          total_errors += int(testsuite.get('errors', 0))
                          total_skipped += int(testsuite.get('skipped', 0))
                          total_time += float(testsuite.get('time', 0))
                  else:  # root is testsuite
                      total_tests += int(root.get('tests', 0))
                      total_failures += int(root.get('failures', 0))
                      total_errors += int(root.get('errors', 0))
                      total_skipped += int(root.get('skipped', 0))
                      total_time += float(root.get('time', 0))
              except Exception as e:
                  print(f'Error parsing {file}: {e}')

          print(f'- **Total Tests**: {total_tests}')
          print(f'- **Passed**: {total_tests - total_failures - total_errors - total_skipped}')
          print(f'- **Failed**: {total_failures}')
          print(f'- **Errors**: {total_errors}')
          print(f'- **Skipped**: {total_skipped}')
          print(f'- **Total Time**: {total_time:.2f}s')
          " >> $GITHUB_STEP_SUMMARY || echo "Could not parse test results" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "_Generated at: $(date -u '+%Y-%m-%d %H:%M:%S UTC')_" >> $GITHUB_STEP_SUMMARY
