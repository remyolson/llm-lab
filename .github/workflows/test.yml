name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, "3.10", "3.11", "3.12"]
        test-type: [unit, integration, performance, compatibility]
        exclude:
          # Run expensive tests only on latest Python version
          - python-version: 3.9
            test-type: performance
          - python-version: "3.10"
            test-type: performance
          - python-version: "3.11"
            test-type: performance
          - python-version: 3.9
            test-type: compatibility
          - python-version: "3.10"
            test-type: compatibility
          - python-version: "3.11"
            test-type: compatibility

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-cov pytest-xdist pytest-mock

    - name: Install package in development mode
      run: pip install -e .

    - name: Set up test environment
      run: |
        # Create necessary directories
        mkdir -p test_outputs
        mkdir -p benchmark_reports
        mkdir -p compatibility_reports

        # Set environment variables for testing
        echo "PYTEST_CURRENT_TEST=true" >> $GITHUB_ENV
        echo "CI=true" >> $GITHUB_ENV

    - name: Run unit tests
      if: matrix.test-type == 'unit'
      run: |
        pytest tests/unit* tests/providers/ -v \
          --cov=llm_providers \
          --cov=tests \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-fail-under=70 \
          --maxfail=5 \
          --tb=short

    - name: Run integration tests
      if: matrix.test-type == 'integration'
      env:
        # Only run integration tests if API keys are available
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        RUN_INTEGRATION_TESTS: ${{ secrets.OPENAI_API_KEY != '' || secrets.ANTHROPIC_API_KEY != '' || secrets.GOOGLE_API_KEY != '' }}
      run: |
        if [ "$RUN_INTEGRATION_TESTS" = "true" ]; then
          echo "Running integration tests with available API keys..."
          python tests/integration/run_integration_tests.py --quick --timeout=30
        else
          echo "Skipping integration tests - no API keys available"
          pytest tests/integration/ -k "not real_api" -v --maxfail=3
        fi

    - name: Run performance benchmarks
      if: matrix.test-type == 'performance'
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        BENCHMARK_MODE: quick
        RUN_EXPENSIVE_BENCHMARKS: false
        RUN_STRESS_TESTS: false
      run: |
        if [ -n "$OPENAI_API_KEY" ] || [ -n "$ANTHROPIC_API_KEY" ]; then
          echo "Running performance benchmarks in quick mode..."
          python tests/performance/demo_performance_suite.py --mode quick --output-dir benchmark_reports
        else
          echo "Skipping performance benchmarks - no API keys available"
          pytest tests/performance/ -k "not real_api" -v --maxfail=3
        fi

    - name: Run compatibility tests
      if: matrix.test-type == 'compatibility'
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      run: |
        if [ -n "$OPENAI_API_KEY" ] || [ -n "$ANTHROPIC_API_KEY" ]; then
          echo "Running compatibility tests..."
          python tests/compatibility/compatibility_runner.py --quick --output-dir compatibility_reports
        else
          echo "Running mocked compatibility tests..."
          pytest tests/compatibility/ -k "not real_api" -v --maxfail=3
        fi

    - name: Upload coverage reports
      if: matrix.test-type == 'unit'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-reports-${{ matrix.python-version }}-${{ matrix.test-type }}
        path: |
          htmlcov/
          benchmark_reports/
          compatibility_reports/
          test_outputs/
        retention-days: 7

    - name: Upload coverage to GitHub
      if: matrix.test-type == 'unit' && matrix.python-version == '3.11'
      run: |
        # Generate coverage badge data
        coverage json
        python -c "
        import json
        with open('coverage.json') as f:
            data = json.load(f)
            print(f'Coverage: {data[\"totals\"][\"percent_covered\"]:.1f}%')
        "

  lint:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install linting dependencies
      run: |
        python -m pip install --upgrade pip
        pip install ruff mypy
        pip install -r requirements.txt

    - name: Run Ruff linter
      run: |
        ruff check .

    - name: Run Ruff formatter check
      run: |
        ruff format --check .

    - name: Run mypy (type checking)
      run: |
        mypy src/ tests/ --ignore-missing-imports --no-strict-optional
      continue-on-error: true  # Don't fail CI on type errors for now

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install security scanning tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety
        pip install -r requirements.txt

    - name: Run bandit (security linting)
      run: |
        bandit -r llm_providers -f json -o bandit-report.json
      continue-on-error: true

    - name: Run safety (dependency vulnerability check)
      run: |
        safety check --json --output safety-report.json
      continue-on-error: true

    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  test-summary:
    needs: [test, lint, security]
    runs-on: ubuntu-latest
    if: always()
    steps:
    - name: Test Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Check test job results
        if [ "${{ needs.test.result }}" = "success" ]; then
          echo "✅ **Tests**: All test suites passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Tests**: Some test suites failed" >> $GITHUB_STEP_SUMMARY
        fi

        # Check lint job results
        if [ "${{ needs.lint.result }}" = "success" ]; then
          echo "✅ **Linting**: Code style and formatting checks passed" >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ **Linting**: Code style issues found" >> $GITHUB_STEP_SUMMARY
        fi

        # Check security job results
        if [ "${{ needs.security.result }}" = "success" ]; then
          echo "✅ **Security**: No security issues detected" >> $GITHUB_STEP_SUMMARY
        else
          echo "⚠️ **Security**: Security scan completed with warnings" >> $GITHUB_STEP_SUMMARY
        fi

        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📊 Detailed reports are available in the job artifacts." >> $GITHUB_STEP_SUMMARY
