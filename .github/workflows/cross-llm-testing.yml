name: Cross-LLM Testing and Performance Analysis

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
    types: [opened, synchronize, reopened, ready_for_review]
  schedule:
    # Run daily at 6 AM UTC for regression monitoring
    - cron: '0 6 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - unit_tests
          - regression_tests
          - performance_tests
          - integration_tests
      provider_filter:
        description: 'Comma-separated list of providers to test (openai,anthropic,google)'
        required: false
        default: 'openai,anthropic,google'
      enable_real_api_tests:
        description: 'Enable tests with real API calls (uses API quotas)'
        required: false
        default: false
        type: boolean
      performance_duration:
        description: 'Performance test duration in minutes'
        required: false
        default: '2'

env:
  PYTHONPATH: ${{ github.workspace }}
  INTEGRATION_TESTS: ${{ github.event.inputs.enable_real_api_tests || 'false' }}
  PERFORMANCE_DURATION: ${{ github.event.inputs.performance_duration || '2' }}

jobs:
  # Job 1: Setup and Code Quality
  setup-and-lint:
    runs-on: ubuntu-latest
    outputs:
      python-versions: ${{ steps.setup.outputs.python-versions }}
      test-suite: ${{ steps.setup.outputs.test-suite }}
      providers: ${{ steps.setup.outputs.providers }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v5
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install pytest-html pytest-json-report pytest-cov

    - name: Setup test configuration
      id: setup
      run: |
        # Determine Python versions to test
        if [[ "${{ github.event_name }}" == "schedule" ]] || [[ "${{ github.event.inputs.test_suite }}" == "all" ]]; then
          echo "python-versions=[\"3.9\", \"3.10\", \"3.11\", \"3.12\"]" >> $GITHUB_OUTPUT
        else
          echo "python-versions=[\"3.11\"]" >> $GITHUB_OUTPUT
        fi

        # Determine test suite
        TEST_SUITE="${{ github.event.inputs.test_suite || 'all' }}"
        echo "test-suite=$TEST_SUITE" >> $GITHUB_OUTPUT

        # Parse provider filter
        PROVIDERS="${{ github.event.inputs.provider_filter || 'openai,anthropic,google' }}"
        echo "providers=$PROVIDERS" >> $GITHUB_OUTPUT

        echo "Configuration:"
        echo "  Test Suite: $TEST_SUITE"
        echo "  Providers: $PROVIDERS"
        echo "  Real API Tests: ${{ env.INTEGRATION_TESTS }}"

    - name: Run Ruff linter
      run: |
        ruff check . --output-format=github

    - name: Run Ruff formatter check
      run: |
        ruff format --check .

    - name: Validate test examples
      run: |
        # Check that our test examples are valid Python
        python -m py_compile examples/use_cases/cross_llm_testing_examples.py
        python -m py_compile examples/use_cases/regression_testing_suite.py
        python -m py_compile examples/use_cases/performance_benchmark_suite.py
        echo "âœ… All test example files are valid Python"

  # Job 2: Unit Tests with Cross-Provider Fixtures
  unit-tests:
    needs: setup-and-lint
    runs-on: ubuntu-latest
    if: contains(fromJSON('["all", "unit_tests"]'), needs.setup-and-lint.outputs.test-suite)
    strategy:
      fail-fast: false
      matrix:
        python-version: ${{ fromJSON(needs.setup-and-lint.outputs.python-versions) }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-${{ matrix.python-version }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install pytest-html pytest-json-report pytest-cov

    - name: Run cross-provider unit tests
      run: |
        # Run the unit tests from our examples
        pytest examples/use_cases/cross_llm_testing_examples.py::TestChatbotCrossProvider \
          -v \
          --cov=examples/use_cases \
          --cov-report=xml \
          --cov-report=html \
          --html=reports/unit-tests-${{ matrix.python-version }}.html \
          --json-report --json-report-file=reports/unit-tests-${{ matrix.python-version }}.json \
          -k "not integration"

    - name: Run existing provider tests
      run: |
        # Run existing test suite
        pytest tests/providers/ \
          -v \
          --cov=src/providers \
          --cov-append \
          --cov-report=xml \
          --cov-report=html \
          -k "not integration"

    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          reports/
          htmlcov/
          coverage.xml

    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.11'
      uses: codecov/codecov-action@v4
      with:
        file: ./coverage.xml
        flags: unit-tests
        name: codecov-unit-tests
        fail_ci_if_error: false

  # Job 3: Integration Tests with Real APIs
  integration-tests:
    needs: [setup-and-lint, unit-tests]
    runs-on: ubuntu-latest
    if: |
      (contains(fromJSON('["all", "integration_tests"]'), needs.setup-and-lint.outputs.test-suite)) &&
      (github.event.inputs.enable_real_api_tests == 'true' || github.event_name == 'schedule')
    strategy:
      fail-fast: false
      matrix:
        provider: ['openai', 'anthropic', 'google']

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-integration-pip-${{ hashFiles('requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install pytest-html pytest-json-report

    - name: Run integration tests for ${{ matrix.provider }}
      env:
        OPENAI_API_KEY: ${{ matrix.provider == 'openai' && secrets.OPENAI_API_KEY || '' }}
        ANTHROPIC_API_KEY: ${{ matrix.provider == 'anthropic' && secrets.ANTHROPIC_API_KEY || '' }}
        GOOGLE_API_KEY: ${{ matrix.provider == 'google' && secrets.GOOGLE_API_KEY || '' }}
        INTEGRATION_TESTS: 'true'
      run: |
        # Skip if API key not available
        case "${{ matrix.provider }}" in
          "openai")
            if [ -z "$OPENAI_API_KEY" ]; then
              echo "âš ï¸ OpenAI API key not available, skipping integration tests"
              exit 0
            fi
            ;;
          "anthropic")
            if [ -z "$ANTHROPIC_API_KEY" ]; then
              echo "âš ï¸ Anthropic API key not available, skipping integration tests"
              exit 0
            fi
            ;;
          "google")
            if [ -z "$GOOGLE_API_KEY" ]; then
              echo "âš ï¸ Google API key not available, skipping integration tests"
              exit 0
            fi
            ;;
        esac

        echo "ðŸš€ Running integration tests for ${{ matrix.provider }}"

        # Run integration tests from examples
        pytest examples/use_cases/cross_llm_testing_examples.py::TestChatbotIntegration \
          -v \
          --html=reports/integration-${{ matrix.provider }}.html \
          --json-report --json-report-file=reports/integration-${{ matrix.provider }}.json \
          -m integration \
          --tb=short \
          --durations=10

        # Run existing integration tests
        pytest tests/integration/ \
          -v \
          --html=reports/integration-existing-${{ matrix.provider }}.html \
          -k "${{ matrix.provider }}" \
          --tb=short \
          || echo "âš ï¸ Some existing integration tests failed (non-blocking)"

    - name: Upload integration test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results-${{ matrix.provider }}
        path: reports/

  # Job 4: Regression Testing Suite
  regression-tests:
    needs: [setup-and-lint, unit-tests]
    runs-on: ubuntu-latest
    if: |
      contains(fromJSON('["all", "regression_tests"]'), needs.setup-and-lint.outputs.test-suite) ||
      github.event_name == 'schedule'

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-regression-pip-${{ hashFiles('requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install pytest-html pytest-json-report
        pip install scipy matplotlib seaborn  # Additional deps for regression suite

    - name: Create regression database directory
      run: |
        mkdir -p regression_data
        chmod 755 regression_data

    - name: Run regression test suite
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        INTEGRATION_TESTS: ${{ github.event.inputs.enable_real_api_tests || 'false' }}
      run: |
        echo "ðŸ”„ Running regression test suite"

        # Run regression tests from examples
        pytest examples/use_cases/regression_testing_suite.py::TestRegressionSuite \
          -v \
          --html=reports/regression-tests.html \
          --json-report --json-report-file=reports/regression-tests.json \
          --tb=short \
          --durations=10

        echo "âœ… Regression test suite completed"

    - name: Generate regression report
      if: always()
      run: |
        python -c "
        import json
        from datetime import datetime

        try:
            with open('reports/regression-tests.json', 'r') as f:
                data = json.load(f)

            print('\nðŸ“Š REGRESSION TEST SUMMARY')
            print('=' * 50)
            print(f'Total Tests: {data[\"summary\"][\"total\"]}')
            print(f'Passed: {data[\"summary\"][\"passed\"]}')
            print(f'Failed: {data[\"summary\"][\"failed\"]}')
            print(f'Skipped: {data[\"summary\"][\"skipped\"]}')
            print(f'Duration: {data[\"duration\"]:.2f}s')

            if data['summary']['failed'] > 0:
                print('\nâŒ Failed Tests:')
                for test in data['tests']:
                    if test['outcome'] == 'failed':
                        print(f'  - {test[\"nodeid\"]}: {test.get(\"call\", {}).get(\"longrepr\", \"Unknown error\")[:100]}...')

        except FileNotFoundError:
            print('No regression test results found')
        except Exception as e:
            print(f'Error processing regression results: {e}')
        "

    - name: Upload regression test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: regression-test-results
        path: |
          reports/
          regression_data/

  # Job 5: Performance Benchmarking
  performance-tests:
    needs: [setup-and-lint, unit-tests]
    runs-on: ubuntu-latest
    if: |
      contains(fromJSON('["all", "performance_tests"]'), needs.setup-and-lint.outputs.test-suite) ||
      github.event_name == 'schedule'

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Cache pip dependencies
      uses: actions/cache@v4
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-performance-pip-${{ hashFiles('requirements*.txt') }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
        pip install pytest-html pytest-json-report
        pip install numpy scipy matplotlib seaborn pandas psutil  # Performance testing deps

    - name: Run performance benchmark tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        INTEGRATION_TESTS: ${{ github.event.inputs.enable_real_api_tests || 'false' }}
        PERFORMANCE_DURATION: ${{ env.PERFORMANCE_DURATION }}
      run: |
        echo "âš¡ Running performance benchmark suite"
        echo "Performance test duration: $PERFORMANCE_DURATION minutes"

        # Run performance tests from examples
        pytest examples/use_cases/performance_benchmark_suite.py::TestPerformanceBenchmark \
          -v \
          --html=reports/performance-tests.html \
          --json-report --json-report-file=reports/performance-tests.json \
          --tb=short \
          --durations=20 \
          -s  # Don't capture output so we can see progress

    - name: Generate performance comparison report
      if: always()
      run: |
        python -c "
        import json
        import os
        from datetime import datetime

        print('\nâš¡ PERFORMANCE TEST SUMMARY')
        print('=' * 50)

        try:
            with open('reports/performance-tests.json', 'r') as f:
                data = json.load(f)

            print(f'Total Tests: {data[\"summary\"][\"total\"]}')
            print(f'Passed: {data[\"summary\"][\"passed\"]}')
            print(f'Failed: {data[\"summary\"][\"failed\"]}')
            print(f'Duration: {data[\"duration\"]:.2f}s')

            # Look for benchmark result files
            benchmark_dir = 'demo_benchmarks'
            if os.path.exists(benchmark_dir):
                benchmark_files = [f for f in os.listdir(benchmark_dir) if f.endswith('.json')]
                if benchmark_files:
                    print(f'\nðŸ“Š Generated {len(benchmark_files)} benchmark result files')
                    for f in benchmark_files:
                        print(f'  - {f}')

        except FileNotFoundError:
            print('No performance test results found')
        except Exception as e:
            print(f'Error processing performance results: {e}')
        "

    - name: Upload performance test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-test-results
        path: |
          reports/
          demo_benchmarks/
          benchmark_results/

  # Job 6: Generate Comprehensive Test Report
  generate-report:
    needs: [setup-and-lint, unit-tests, integration-tests, regression-tests, performance-tests]
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v5

    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-artifacts

    - name: Generate comprehensive test report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        from pathlib import Path

        print('ðŸ“‹ COMPREHENSIVE CROSS-LLM TEST REPORT')
        print('=' * 60)
        print(f'Generated: {datetime.now().isoformat()}')
        print(f'Trigger: ${{ github.event_name }}')
        print(f'Ref: ${{ github.ref }}')
        print(f'SHA: ${{ github.sha }}')
        print()

        artifacts_dir = Path('test-artifacts')
        report_sections = []

        # Process each test type
        test_types = {
            'Unit Tests': 'unit-test-results',
            'Integration Tests': 'integration-test-results',
            'Regression Tests': 'regression-test-results',
            'Performance Tests': 'performance-test-results'
        }

        for test_name, artifact_pattern in test_types.items():
            print(f'ðŸ“Š {test_name}')
            print('-' * (len(test_name) + 4))

            # Find matching artifacts
            matching_dirs = [d for d in artifacts_dir.iterdir()
                           if d.is_dir() and artifact_pattern in d.name]

            if not matching_dirs:
                print(f'  âŒ No results found')
                continue

            for result_dir in matching_dirs:
                json_files = list(result_dir.glob('*.json'))
                if not json_files:
                    continue

                for json_file in json_files:
                    if 'report' in json_file.name.lower():
                        continue  # Skip report files

                    try:
                        with open(json_file) as f:
                            data = json.load(f)

                        if 'summary' in data:
                            summary = data['summary']
                            print(f'  ðŸ“ {json_file.parent.name}:')
                            print(f'     Total: {summary.get(\"total\", 0)}')
                            print(f'     Passed: {summary.get(\"passed\", 0)}')
                            print(f'     Failed: {summary.get(\"failed\", 0)}')
                            print(f'     Duration: {data.get(\"duration\", 0):.2f}s')

                            if summary.get('failed', 0) > 0:
                                print(f'     âš ï¸  {summary[\"failed\"]} test(s) failed')

                    except Exception as e:
                        print(f'  âŒ Error reading {json_file}: {e}')

            print()

        # Overall status
        print('ðŸŽ¯ OVERALL STATUS')
        print('-' * 16)

        # Check if any critical tests failed
        failed_jobs = []
        if '${{ needs.unit-tests.result }}' == 'failure':
            failed_jobs.append('Unit Tests')
        if '${{ needs.integration-tests.result }}' == 'failure':
            failed_jobs.append('Integration Tests')
        if '${{ needs.regression-tests.result }}' == 'failure':
            failed_jobs.append('Regression Tests')
        if '${{ needs.performance-tests.result }}' == 'failure':
            failed_jobs.append('Performance Tests')

        if failed_jobs:
            print(f'âŒ Failed Jobs: {', '.join(failed_jobs)}')
        else:
            print('âœ… All test jobs completed successfully')

        print()
        print('ðŸ“ RECOMMENDATIONS')
        print('-' * 17)
        if failed_jobs:
            print('1. Review failed test details in individual job logs')
            print('2. Check API key configuration for integration tests')
            print('3. Verify provider compatibility for any new changes')
        else:
            print('1. All cross-LLM tests passed - good to merge!')
            print('2. Consider running performance tests on main branch')
            print('3. Monitor regression trends in scheduled runs')
        "

    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          // Create a summary comment for the PR
          let comment = '## ðŸ§ª Cross-LLM Test Results\\n\\n';
          comment += `**Trigger:** ${context.eventName}\\n`;
          comment += `**Commit:** ${context.sha.substring(0, 7)}\\n\\n`;

          // Job status
          const jobs = {
            'Unit Tests': '${{ needs.unit-tests.result }}',
            'Integration Tests': '${{ needs.integration-tests.result }}',
            'Regression Tests': '${{ needs.regression-tests.result }}',
            'Performance Tests': '${{ needs.performance-tests.result }}'
          };

          comment += '### Job Status\\n';
          for (const [job, status] of Object.entries(jobs)) {
            const icon = status === 'success' ? 'âœ…' : status === 'failure' ? 'âŒ' : status === 'skipped' ? 'â­ï¸' : 'âš ï¸';
            comment += `- ${icon} **${job}**: ${status}\\n`;
          }

          comment += '\\n### Artifacts\\n';
          comment += 'Test reports and coverage data are available in the workflow artifacts.\\n\\n';

          if (Object.values(jobs).includes('failure')) {
            comment += 'âš ï¸ **Some tests failed.** Please review the logs and fix any issues before merging.\\n';
          } else {
            comment += 'ðŸŽ‰ **All tests passed!** This PR is ready for review.\\n';
          }

          // Find existing comment to update
          const { data: comments } = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });

          const existingComment = comments.find(comment =>
            comment.body.includes('Cross-LLM Test Results')
          );

          if (existingComment) {
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: comment
            });
          } else {
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: comment
            });
          }

    - name: Upload comprehensive report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: comprehensive-test-report
        path: test-artifacts/

  # Job 7: Cleanup and Notifications
  cleanup-and-notify:
    needs: [generate-report]
    runs-on: ubuntu-latest
    if: always() && github.event_name == 'schedule'

    steps:
    - name: Send daily regression summary
      if: github.event_name == 'schedule'
      run: |
        echo "ðŸ“§ Daily regression test summary would be sent here"
        echo "In a real implementation, this would:"
        echo "1. Parse all test results"
        echo "2. Generate trend analysis"
        echo "3. Send email/Slack notification with summary"
        echo "4. Update regression tracking dashboard"

        # Example of what could be implemented:
        # curl -X POST -H 'Content-type: application/json' \
        #   --data '{"text":"Daily Cross-LLM Test Summary: ..."}' \
        #   $SLACK_WEBHOOK_URL
