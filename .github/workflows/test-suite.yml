name: Comprehensive Test Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

jobs:
  test-unit:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-xdist pytest-mock
        pip install coverage[toml]
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Run unit tests with coverage
      run: |
        pytest tests/unit_providers/ tests/providers/ \
          --cov=llm_providers \
          --cov=tests \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junitxml=junit/test-results-${{ matrix.python-version }}.xml \
          -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-${{ matrix.python-version }}
        fail_ci_if_error: false
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}
        path: |
          junit/test-results-*.xml
          htmlcov/
    
    - name: Archive coverage reports
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report-${{ matrix.python-version }}
        path: htmlcov/

  test-integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[run-integration]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Run integration tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        RUN_INTEGRATION_TESTS: true
      run: |
        pytest tests/integration/ \
          --cov=llm_providers \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=junit/integration-results.xml \
          -v -s --tb=short
    
    - name: Upload integration coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration
        name: codecov-integration
        fail_ci_if_error: false
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: junit/integration-results.xml

  test-compatibility:
    name: Compatibility Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[run-compatibility]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Run compatibility tests
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        pytest tests/compatibility/ \
          --cov=llm_providers \
          --cov-report=xml \
          --cov-report=term-missing \
          --junitxml=junit/compatibility-results.xml \
          -v --tb=short
    
    - name: Generate compatibility report
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
      run: |
        python tests/compatibility/demo_compatibility_suite.py \
          --providers all \
          --output-dir compatibility_reports \
          --format both
    
    - name: Upload compatibility coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: compatibility
        name: codecov-compatibility
        fail_ci_if_error: false
    
    - name: Upload compatibility reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: compatibility-reports
        path: |
          junit/compatibility-results.xml
          compatibility_reports/

  test-performance:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || contains(github.event.head_commit.message, '[run-benchmarks]')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest matplotlib seaborn pandas psutil
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
        if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
    
    - name: Run performance benchmarks
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
        GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}
        BENCHMARK_MODE: quick
        RUN_EXPENSIVE_BENCHMARKS: false
      run: |
        python tests/performance/demo_performance_suite.py \
          --mode quick \
          --providers all \
          --output-dir benchmark_reports
    
    - name: Upload benchmark reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: benchmark-reports
        path: benchmark_reports/

  test-security:
    name: Security and Code Quality
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep
        pip install ruff mypy
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    
    - name: Run Bandit security linter
      run: bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Run Safety check
      run: safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Run Semgrep
      run: semgrep --config=auto --json --output=semgrep-report.json src/
      continue-on-error: true
    
    - name: Run Ruff linter
      run: ruff check .
      continue-on-error: true
    
    - name: Run Ruff formatter check
      run: ruff format --check .
      continue-on-error: true
    
    - name: Run mypy type checking
      run: mypy src/ tests/ --ignore-missing-imports --no-strict-optional
      continue-on-error: true
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
          semgrep-report.json

  test-coverage-report:
    name: Generate Coverage Report
    runs-on: ubuntu-latest
    needs: [test-unit, test-integration, test-compatibility]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install coverage tools
      run: |
        python -m pip install --upgrade pip
        pip install coverage[toml] pytest
    
    - name: Download coverage artifacts
      uses: actions/download-artifact@v4
      with:
        path: coverage-artifacts/
    
    - name: Combine coverage reports
      run: |
        # Find all coverage files and combine them
        find coverage-artifacts/ -name "*.xml" -type f | head -5 | while read file; do
          echo "Processing coverage file: $file"
          cp "$file" "./coverage-$(basename $(dirname $file)).xml" 2>/dev/null || true
        done
        
        # Generate combined HTML report
        coverage html --skip-covered --skip-empty || true
        coverage report --skip-covered || true
    
    - name: Upload combined coverage
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage-*.xml
        flags: combined
        name: codecov-combined
        fail_ci_if_error: false
    
    - name: Generate coverage badge
      run: |
        # Simple coverage badge generation
        python -c "
        import json, os
        try:
            import coverage
            cov = coverage.Coverage()
            cov.load()
            total = cov.report(skip_covered=False, skip_empty=False, show_missing=False)
            color = 'brightgreen' if total >= 90 else 'green' if total >= 80 else 'yellow' if total >= 70 else 'red'
            badge_data = {
                'schemaVersion': 1,
                'label': 'coverage',
                'message': f'{total:.1f}%',
                'color': color
            }
            with open('coverage-badge.json', 'w') as f:
                json.dump(badge_data, f)
            print(f'Coverage: {total:.1f}%')
        except Exception as e:
            print(f'Could not generate coverage badge: {e}')
        " || echo "Coverage calculation failed"
    
    - name: Upload final coverage report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: final-coverage-report
        path: |
          htmlcov/
          coverage-badge.json

  notify-results:
    name: Notify Test Results
    runs-on: ubuntu-latest
    needs: [test-unit, test-integration, test-compatibility, test-performance, test-security]
    if: always() && (github.event_name == 'schedule' || github.ref == 'refs/heads/main')
    
    steps:
    - name: Determine overall status
      id: status
      run: |
        # Check if any job failed
        if [[ "${{ needs.test-unit.result }}" == "failure" ]] || \
           [[ "${{ needs.test-integration.result }}" == "failure" ]] || \
           [[ "${{ needs.test-compatibility.result }}" == "failure" ]] || \
           [[ "${{ needs.test-security.result }}" == "failure" ]]; then
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "color=red" >> $GITHUB_OUTPUT
        elif [[ "${{ needs.test-unit.result }}" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT  
          echo "color=green" >> $GITHUB_OUTPUT
        else
          echo "status=partial" >> $GITHUB_OUTPUT
          echo "color=yellow" >> $GITHUB_OUTPUT
        fi
    
    - name: Create test summary
      run: |
        echo "# Test Suite Results 🧪" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status | " >> $GITHUB_STEP_SUMMARY
        echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.test-unit.result == 'success' && '✅ Passed' || needs.test-unit.result == 'failure' && '❌ Failed' || '⏭️ Skipped' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.test-integration.result == 'success' && '✅ Passed' || needs.test-integration.result == 'failure' && '❌ Failed' || '⏭️ Skipped' }} |" >> $GITHUB_STEP_SUMMARY  
        echo "| Compatibility Tests | ${{ needs.test-compatibility.result == 'success' && '✅ Passed' || needs.test-compatibility.result == 'failure' && '❌ Failed' || '⏭️ Skipped' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance Benchmarks | ${{ needs.test-performance.result == 'success' && '✅ Passed' || needs.test-performance.result == 'failure' && '❌ Failed' || '⏭️ Skipped' }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security & Code Quality | ${{ needs.test-security.result == 'success' && '✅ Passed' || needs.test-security.result == 'failure' && '❌ Failed' || '⏭️ Skipped' }} |" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Overall Status:** ${{ steps.status.outputs.status == 'success' && '🟢 All tests passed!' || steps.status.outputs.status == 'failure' && '🔴 Some tests failed' || '🟡 Partial test execution' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "_Run triggered by: ${{ github.event_name }} on ${{ github.ref }}_" >> $GITHUB_STEP_SUMMARY