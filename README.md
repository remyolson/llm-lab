# LLM Lab - Comprehensive Large Language Model Research Platform

A professional-grade framework for benchmarking, fine-tuning, monitoring, and aligning Large Language Models (LLMs) across multiple providers with enterprise features.

## üöÄ Overview

LLM Lab is a comprehensive platform designed for organizations and researchers working with Large Language Models. It provides production-ready tools for evaluation, optimization, safety alignment, and continuous monitoring of LLM deployments.

### Key Features

- **Multi-Provider Support**: Unified interface for OpenAI, Anthropic, Google, Azure, Cohere, Mistral, and local models
- **Comprehensive Benchmarking**: Multi-dataset evaluation with statistical analysis and confidence intervals
- **Fine-Tuning Pipelines**: Support for both cloud-based and local model fine-tuning (LoRA, QLoRA, full)
- **Advanced Alignment**: Constitutional AI, multi-layer safety filters, and preference learning (RLHF)
- **Real-Time Monitoring**: Production monitoring with dashboards, alerts, and SLA tracking
- **Cost Management**: Detailed cost tracking, forecasting, and optimization recommendations
- **Enterprise Features**: Multi-tenancy, custom evaluators, A/B testing, and compliance reporting
- **Production Ready**: 95%+ test coverage, comprehensive error handling, and extensive documentation

## üìã Prerequisites

- Python 3.9 or higher
- API keys for desired LLM providers (OpenAI, Anthropic, Google, etc.)
- pip package manager
- (Optional) CUDA-capable GPU for local model fine-tuning
- (Optional) Docker for containerized deployments

## üõ†Ô∏è Installation

### Quick Setup

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/lllm-lab.git
   cd lllm-lab
   ```

2. **Create and activate a virtual environment**
   
   On macOS/Linux:
   ```bash
   python3 -m venv venv
   source venv/bin/activate
   ```
   
   On Windows:
   ```cmd
   python -m venv venv
   venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Download models and datasets**
   ```bash
   # Download everything (recommended for first-time setup)
   python download_assets.py --all

   # Or download selectively:
   python download_assets.py --models      # Only models
   python download_assets.py --datasets    # Only datasets
   python download_assets.py --list        # See what's available
   ```

### üì• Asset Management

This repository uses a download-based approach for large files (models and datasets) instead of Git LFS to ensure:
- No bandwidth costs for contributors
- Easy setup for forks and clones
- Better performance for large files

**Available download options:**
- `--all`: Download everything (~2.5GB)
- `--models`: Download all models (~2.4GB)
- `--datasets`: Download all datasets (~10MB)
- `--model <name>`: Download specific model (e.g., `qwen-0.5b`)
- `--dataset <name>`: Download specific dataset (e.g., `truthfulqa-full`)
- `--verify`: Check that all downloaded files are valid

**Note:** The repository includes small sample files for development/testing. Large model files and complete datasets are downloaded separately.

## ‚öôÔ∏è Configuration

1. **Copy the example environment file**
   ```bash
   cp .env.example .env
   ```

2. **Add your API keys**
   
   Edit `.env` and add the API keys for providers you want to use:
   ```
   # Core Providers
   OPENAI_API_KEY=your-openai-key-here
   ANTHROPIC_API_KEY=your-anthropic-key-here
   GOOGLE_API_KEY=your-google-key-here
   
   # Additional Providers (optional)
   AZURE_OPENAI_ENDPOINT=your-azure-endpoint
   AZURE_OPENAI_API_KEY=your-azure-key
   COHERE_API_KEY=your-cohere-key
   MISTRAL_API_KEY=your-mistral-key
   PERPLEXITY_API_KEY=your-perplexity-key
   
   # Monitoring (optional)
   SLACK_WEBHOOK_URL=your-slack-webhook
   SMTP_SERVER=smtp.gmail.com
   SMTP_USER=your-email@gmail.com
   SMTP_PASSWORD=your-app-password
   ```

   API Key Resources:
   - OpenAI: [platform.openai.com](https://platform.openai.com/api-keys)
   - Anthropic: [console.anthropic.com](https://console.anthropic.com/)
   - Google: [makersuite.google.com](https://makersuite.google.com/app/apikey)
   - Azure: [portal.azure.com](https://portal.azure.com/)
   - Cohere: [dashboard.cohere.ai](https://dashboard.cohere.ai/)
   - Mistral: [console.mistral.ai](https://console.mistral.ai/)

## üéÆ Usage

### Use Case 1: Multi-Provider Benchmarking

Run comprehensive benchmarks across multiple providers:

```bash
# Basic benchmark
python src/use_cases/multi_provider_benchmarking.py

# With specific providers and models
python src/use_cases/multi_provider_benchmarking.py \
    --providers openai anthropic google \
    --models gpt-4 claude-3-5-sonnet-20241022 gemini-1.5-pro \
    --datasets truthfulness gsm8k humaneval \
    --output-format html pdf csv
```

### Use Case 2: Cost-Optimized Model Selection

Analyze cost vs performance trade-offs:

```bash
python src/use_cases/cost_optimization.py \
    --budget 1000 \
    --min-accuracy 0.9 \
    --workload-profile production
```

### Use Case 3: A/B Testing

Compare models in production:

```bash
python src/use_cases/ab_testing.py \
    --model-a gpt-4 \
    --model-b claude-3-5-sonnet-20241022 \
    --traffic-split 50/50 \
    --duration 7d
```

### Use Case 4: Custom Evaluation Pipelines

Create domain-specific evaluations:

```bash
python src/use_cases/custom_evaluation.py \
    --evaluator medical \
    --test-suite clinical-notes \
    --compliance hipaa
```

### Use Case 5: Production Monitoring

Monitor deployed models:

```bash
python src/use_cases/monitoring/dashboard.py \
    --models production-models.yaml \
    --port 8080 \
    --alerts slack email
```

### Use Case 6: Model Fine-Tuning

Fine-tune models on custom data:

```bash
# Cloud fine-tuning
python src/use_cases/fine_tuning.py \
    --provider openai \
    --base-model gpt-3.5-turbo \
    --dataset custom-data.jsonl \
    --epochs 3

# Local fine-tuning
python src/use_cases/fine_tuning.py \
    --local \
    --model llama-2-7b \
    --method lora \
    --dataset custom-data.json
```

### Use Case 7: Alignment Research

Implement safety measures:

```bash
python src/use_cases/alignment.py \
    --rules constitutional-ai.yaml \
    --safety-filters strict \
    --consensus-models 3
```

### Use Case 8: Automated Reporting

Generate comprehensive reports:

```bash
python src/use_cases/reporting.py \
    --schedule daily \
    --recipients team@company.com \
    --include costs performance safety compliance

### Example Output

```
üî¨ LLM Lab Benchmark Runner
==================================================

1. Loading configuration...
   ‚úì Model configuration loaded
   ‚úì API key loaded for google

2. Initializing provider...
   ‚úì google provider initialized

3. Loading dataset...
   ‚úì Dataset validated
   ‚úì Loaded 1 prompts from truthfulness

4. Running evaluations...
   Prompt 1/1: Who wrote 'Don Quixote'?...
   Response: Miguel de Cervantes wrote Don Quixote...
   ‚úì Evaluation passed (matched: Cervantes, Miguel de Cervantes)

==================================================
üìä Benchmark Results Summary
==================================================
Provider: google
Dataset: truthfulness
Total Prompts: 1
Successful: 1
Failed: 0
Overall Score: 100.00%
```

## üìä Results and Reporting

### Enhanced Results Logging

Results are automatically saved with organized structure and enhanced file naming:

```
results/
‚îú‚îÄ‚îÄ truthfulness/                    # Dataset-based organization
‚îÇ   ‚îî‚îÄ‚îÄ 2024-12/                    # Year-month organization
‚îÇ       ‚îú‚îÄ‚îÄ benchmark_google_gemini-1-5-flash_truthfulness_20241231_143022.csv
‚îÇ       ‚îú‚îÄ‚îÄ benchmark_openai_gpt-4o-mini_truthfulness_20241231_143022.csv
‚îÇ       ‚îú‚îÄ‚îÄ benchmark_anthropic_claude-3-haiku_truthfulness_20241231_143022.csv
‚îÇ       ‚îú‚îÄ‚îÄ results_index.json       # Metadata index
‚îÇ       ‚îî‚îÄ‚îÄ performance_analysis_20241231_143022.html
‚îî‚îÄ‚îÄ performance_benchmarks/
    ‚îî‚îÄ‚îÄ 2024-12/
        ‚îú‚îÄ‚îÄ performance_report_20241231_143022.json
        ‚îî‚îÄ‚îÄ benchmark_charts_20241231_143022.png
```

### CSV Results Format

Results are saved in CSV format with enhanced columns:

| Column | Description |
|--------|-------------|
| timestamp | ISO format timestamp of the evaluation |
| provider | Provider name (google, openai, anthropic) |
| model_name | Full model identifier (e.g., google/gemini-1.5-flash) |
| model_version | Model version/variant |
| benchmark_name | Name of the benchmark dataset |
| prompt_id | Unique identifier for the prompt |
| prompt_text | The input prompt text |
| model_response | The model's generated response |
| expected_keywords | JSON array of expected keywords |
| matched_keywords | JSON array of matched keywords |
| score | Evaluation score (0.0 to 1.0) |
| success | Pass/fail status |
| evaluation_method | Method used for evaluation |
| response_time_seconds | Time taken to generate response |
| tokens_used | Number of tokens consumed (if available) |
| cost_estimate | Estimated cost in USD (if available) |
| error | Any error message (if applicable) |
| session_id | Unique session identifier for grouping |
| run_metadata | Additional run configuration (JSON) |

### Performance Analysis Reports

When `--analyze-results` is used, additional reports are generated:

- **HTML Performance Dashboard**: Interactive charts and analysis
- **Statistical Summary**: Response time distributions, confidence intervals
- **Comparison Matrix**: Head-to-head model comparisons
- **Cost Analysis**: Token usage and cost estimates by provider
- **Error Analysis**: Failure patterns and retry statistics

## üìÅ Project Structure

```
lllm-lab/
‚îú‚îÄ‚îÄ src/                     # Source code
‚îÇ   ‚îú‚îÄ‚îÄ llm_providers/       # LLM provider implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py          # Base provider interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.py        # OpenAI GPT models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.py     # Anthropic Claude models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google.py        # Google Gemini models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ azure.py         # Azure OpenAI service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cohere.py        # Cohere models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mistral.py       # Mistral models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ local.py         # Local model support
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/          # Evaluation methods
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py       # Core metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom_evaluators.py # Domain evaluators
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ statistical_analysis.py # Stats tools
‚îÇ   ‚îú‚îÄ‚îÄ use_cases/           # Production use cases
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ multi_provider_benchmarking.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cost_optimization.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ab_testing.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ custom_evaluation.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fine_tuning.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alignment.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring/      # Monitoring subsystem
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics_collector.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alert_manager.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ reporting.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Shared utilities
‚îÇ       ‚îú‚îÄ‚îÄ cost_tracker.py
‚îÇ       ‚îú‚îÄ‚îÄ results_logger.py
‚îÇ       ‚îî‚îÄ‚îÄ config_manager.py
‚îú‚îÄ‚îÄ examples/                # Example implementations
‚îÇ   ‚îú‚îÄ‚îÄ use_cases/           # Use case examples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alignment_demo.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fine_tuning_complete_demo.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ monitoring_complete_demo.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ integrated_workflow_demo.py
‚îÇ   ‚îú‚îÄ‚îÄ custom_prompts/      # Prompt templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alignment_rules.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ safety_filters.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ monitoring_config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/           # Jupyter notebooks
‚îÇ       ‚îî‚îÄ‚îÄ integrated_workflows.ipynb
‚îú‚îÄ‚îÄ benchmarks/              # Benchmark datasets
‚îÇ   ‚îú‚îÄ‚îÄ truthfulness/
‚îÇ   ‚îú‚îÄ‚îÄ gsm8k/
‚îÇ   ‚îú‚îÄ‚îÄ humaneval/
‚îÇ   ‚îî‚îÄ‚îÄ custom/
‚îú‚îÄ‚îÄ datasets/                # Training datasets
‚îÇ   ‚îú‚îÄ‚îÄ benchmarking/
‚îÇ   ‚îî‚îÄ‚îÄ fine-tuning/
‚îú‚îÄ‚îÄ models/                  # Model storage
‚îÇ   ‚îú‚îÄ‚îÄ cloud-configs/       # Cloud model configs
‚îÇ   ‚îî‚îÄ‚îÄ local-models/        # Local model files
‚îú‚îÄ‚îÄ tests/                   # Comprehensive test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_alignment.py
‚îÇ   ‚îú‚îÄ‚îÄ test_fine_tuning.py
‚îÇ   ‚îú‚îÄ‚îÄ test_monitoring.py
‚îÇ   ‚îú‚îÄ‚îÄ test_integrated_workflows.py
‚îÇ   ‚îî‚îÄ‚îÄ ... (40+ test files)
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ guides/              # How-to guides
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ USE_CASE_*.md   # Use case guides
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ README.md        # Guide index
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # API documentation
‚îÇ   ‚îî‚îÄ‚îÄ architecture/        # System design docs
‚îú‚îÄ‚îÄ templates/               # Report templates
‚îÇ   ‚îú‚îÄ‚îÄ email/
‚îÇ   ‚îî‚îÄ‚îÄ pdf/
‚îú‚îÄ‚îÄ .github/                 # GitHub configuration
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îú‚îÄ‚îÄ docker/                  # Docker configurations
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt         # Core dependencies
‚îú‚îÄ‚îÄ requirements-dev.txt     # Development deps
‚îú‚îÄ‚îÄ requirements-gpu.txt     # GPU/ML deps
‚îú‚îÄ‚îÄ pyproject.toml          # Project config
‚îú‚îÄ‚îÄ .env.example            # Environment template
‚îî‚îÄ‚îÄ README.md               # This file
```

### üóÇÔ∏è Asset Organization

- **Sample files**: Small example files are included in the repository for testing
- **Large files**: Models (GB-sized) and full datasets are downloaded via `download_assets.py`
- **Ignore patterns**: Large files are gitignored to keep the repository lightweight

## üß™ Testing

The project includes comprehensive test coverage for all components:

### Running Tests

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_alignment.py      # Alignment tests
pytest tests/test_fine_tuning.py    # Fine-tuning tests
pytest tests/test_monitoring.py     # Monitoring tests
pytest tests/test_integrated_workflows.py  # Integration tests

# Run with coverage report
pytest --cov=. --cov-report=html --cov-report=term

# Run only fast tests (skip integration)
pytest -m "not integration"

# Run with specific provider
pytest --provider=openai
```

### Test Categories

- **Unit Tests**: Individual component testing
- **Integration Tests**: Multi-component workflows
- **Performance Tests**: Latency and throughput benchmarks
- **Safety Tests**: Alignment and filter validation
- **Mock Tests**: No API calls required

### Coverage Report

```bash
# Generate and view coverage report
pytest --cov=src --cov-report=html
open htmlcov/index.html  # macOS
start htmlcov/index.html  # Windows
```

Current coverage: 95%+ across all modules

## üîß Development

### Quick Start Development Guide

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Set up pre-commit hooks
pre-commit install

# Run code quality checks
make lint
make format
make type-check

# Run all checks before committing
make check-all
```

### Adding New Features

#### New LLM Provider

1. Extend `BaseProvider` in `src/llm_providers/base.py`
2. Implement required methods:
   - `generate()` - Text generation
   - `get_model_info()` - Model metadata
   - `estimate_cost()` - Cost calculation
3. Add provider configuration to `config.yaml`
4. Write tests in `tests/test_providers/`

#### New Evaluation Method

1. Create evaluator in `src/evaluation/custom_evaluators.py`
2. Implement `evaluate()` method returning scores
3. Register in `EVALUATOR_REGISTRY`
4. Add example usage to documentation

#### New Use Case

1. Create script in `src/use_cases/`
2. Add corresponding guide in `docs/guides/`
3. Include example in `examples/use_cases/`
4. Write comprehensive tests

### Code Quality Standards

- **Type Hints**: All functions must have type annotations
- **Docstrings**: Google-style docstrings required
- **Testing**: Minimum 90% coverage for new code
- **Linting**: Pass all ruff and mypy checks
- **Documentation**: Update relevant docs with changes

## üöÄ Advanced Features

### Production Deployment

#### Docker Deployment

```bash
# Build and run with Docker
docker build -t llm-lab .
docker run -p 8080:8080 --env-file .env llm-lab

# Using docker-compose for full stack
docker-compose up -d
```

#### Kubernetes Deployment

```yaml
# See kubernetes/ directory for manifests
kubectl apply -f kubernetes/
```

#### Monitoring Stack

- **Grafana Dashboards**: Pre-built dashboards for all metrics
- **Prometheus Integration**: Metrics export in Prometheus format
- **Custom Alerts**: Slack, Email, PagerDuty, SMS support
- **SLA Tracking**: Automated compliance reporting

### Enterprise Features

#### Multi-Tenancy

```python
# Configure tenant isolation
config = {
    "tenants": {
        "team-a": {"models": ["gpt-4"], "budget": 1000},
        "team-b": {"models": ["claude-3-5"], "budget": 2000}
    }
}
```

#### Compliance & Security

- **Data Encryption**: At-rest and in-transit
- **Audit Logging**: Complete activity tracking
- **PII Detection**: Automatic redaction
- **HIPAA/GDPR**: Compliance templates included

#### High Availability

- **Load Balancing**: Automatic request distribution
- **Failover**: Multi-region support
- **Caching**: Response caching with TTL
- **Rate Limiting**: Per-tenant and global limits

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Process

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes following our code standards
4. Write/update tests (maintain 90%+ coverage)
5. Update documentation
6. Run `make check-all` to ensure quality
7. Commit your changes (`git commit -m 'Add amazing feature'`)
8. Push to the branch (`git push origin feature/amazing-feature`)
9. Open a Pull Request with a detailed description

### Areas for Contribution

- üåü New LLM provider integrations
- üìä Additional benchmark datasets
- üîß Performance optimizations
- üìö Documentation improvements
- üåç Internationalization
- üß™ Test coverage expansion

## üìö Resources

### Documentation

- [Full Documentation](https://llm-lab.readthedocs.io)
- [API Reference](docs/api/README.md)
- [Architecture Guide](docs/architecture/README.md)
- [Use Case Guides](docs/guides/README.md)

### Community

- [Discord Server](https://discord.gg/llm-lab)
- [GitHub Discussions](https://github.com/yourusername/llm-lab/discussions)
- [Twitter Updates](https://twitter.com/llm_lab)

### Related Projects

- [LangChain](https://github.com/hwchase17/langchain) - LLM application framework
- [LlamaIndex](https://github.com/jerryjliu/llama_index) - Data framework for LLMs
- [Haystack](https://github.com/deepset-ai/haystack) - NLP framework

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- OpenAI, Anthropic, Google, and other LLM providers for their APIs
- The open-source community for invaluable tools and libraries
- Contributors and users who help improve LLM Lab
- Research papers and benchmarks that inform our evaluations

## üìä Project Status

![GitHub Stars](https://img.shields.io/github/stars/yourusername/llm-lab)
![GitHub Forks](https://img.shields.io/github/forks/yourusername/llm-lab)
![License](https://img.shields.io/github/license/yourusername/llm-lab)
![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)
![Coverage](https://img.shields.io/badge/coverage-95%25-brightgreen)

---

Built with ‚ù§Ô∏è by the LLM Lab Team