{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with LLM Lab\n",
    "\n",
    "This notebook demonstrates the basic usage of LLM Lab for working with multiple LLM providers.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Basic Provider Usage](#basic-provider)\n",
    "3. [Comparing Multiple Models](#comparing-models)\n",
    "4. [Cost Analysis](#cost-analysis)\n",
    "5. [Next Steps](#next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation <a id='setup'></a>\n",
    "\n",
    "First, let's set up the environment and import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import LLM Lab modules\n",
    "from src.analysis.comparator import ModelComparator\n",
    "from src.cost.tracker import CostTracker\n",
    "from src.providers import AnthropicProvider, GoogleProvider, OpenAIProvider\n",
    "\n",
    "print(\"LLM Lab modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Provider Usage <a id='basic-provider'></a>\n",
    "\n",
    "Let's start with a simple example using the OpenAI provider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI provider (replace with your API key)\n",
    "openai_provider = OpenAIProvider(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\", \"your-api-key-here\"), model=\"gpt-3.5-turbo\", temperature=0.7\n",
    ")\n",
    "\n",
    "# Generate a simple response\n",
    "prompt = \"Explain machine learning in 2 sentences.\"\n",
    "response = openai_provider.generate(prompt, max_tokens=100)\n",
    "\n",
    "print(\"Model Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.get(\"content\", \"No response\"))\n",
    "print(\"-\" * 50)\n",
    "print(f\"Tokens used: {response.get('usage', {}).get('total_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Multiple Models <a id='comparing-models'></a>\n",
    "\n",
    "Now let's compare responses from different providers on the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up multiple providers\n",
    "providers = {}\n",
    "\n",
    "# Add OpenAI if API key is available\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    providers[\"OpenAI (GPT-3.5)\"] = OpenAIProvider(\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "\n",
    "# Add Anthropic if API key is available\n",
    "if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "    providers[\"Anthropic (Claude)\"] = AnthropicProvider(\n",
    "        api_key=os.getenv(\"ANTHROPIC_API_KEY\"), model=\"claude-3-haiku-20240307\"\n",
    "    )\n",
    "\n",
    "# Add Google if API key is available\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    providers[\"Google (Gemini)\"] = GoogleProvider(\n",
    "        api_key=os.getenv(\"GOOGLE_API_KEY\"), model=\"gemini-1.5-flash\"\n",
    "    )\n",
    "\n",
    "print(f\"Available providers: {list(providers.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses on a common prompt\n",
    "test_prompt = \"What are the main differences between supervised and unsupervised learning?\"\n",
    "\n",
    "responses = {}\n",
    "for name, provider in providers.items():\n",
    "    try:\n",
    "        response = provider.generate(test_prompt, max_tokens=150)\n",
    "        responses[name] = {\n",
    "            \"content\": response.get(\"content\", \"No response\"),\n",
    "            \"tokens\": response.get(\"usage\", {}).get(\"total_tokens\", 0),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        responses[name] = {\"content\": f\"Error: {str(e)}\", \"tokens\": 0}\n",
    "\n",
    "# Display responses\n",
    "for name, resp in responses.items():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    print(resp[\"content\"][:300] + \"...\" if len(resp[\"content\"]) > 300 else resp[\"content\"])\n",
    "    print(f\"\\nTokens used: {resp['tokens']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis <a id='cost-analysis'></a>\n",
    "\n",
    "Let's analyze the cost of our API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize cost tracker\n",
    "cost_tracker = CostTracker()\n",
    "\n",
    "# Simulate some API calls with cost tracking\n",
    "test_prompts = [\"What is Python?\", \"Explain neural networks.\", \"How does gradient descent work?\"]\n",
    "\n",
    "total_costs = {}\n",
    "for name, provider in providers.items():\n",
    "    provider_cost = 0.0\n",
    "    for prompt in test_prompts:\n",
    "        try:\n",
    "            response = provider.generate(prompt, max_tokens=100)\n",
    "            # Estimate cost (simplified - actual implementation would use provider-specific pricing)\n",
    "            tokens = response.get(\"usage\", {}).get(\"total_tokens\", 0)\n",
    "            cost = cost_tracker.estimate_cost(provider.provider_name, provider.model_name, tokens)\n",
    "            provider_cost += cost\n",
    "        except:\n",
    "            pass\n",
    "    total_costs[name] = provider_cost\n",
    "\n",
    "# Display cost analysis\n",
    "print(\"Cost Analysis Summary\")\n",
    "print(\"=\" * 40)\n",
    "for name, cost in total_costs.items():\n",
    "    print(f\"{name}: ${cost:.4f}\")\n",
    "print(f\"\\nTotal cost: ${sum(total_costs.values()):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results\n",
    "\n",
    "Let's create a simple visualization of token usage across providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data for visualization\n",
    "provider_names = list(responses.keys())\n",
    "token_counts = [resp[\"tokens\"] for resp in responses.values()]\n",
    "\n",
    "# Create bar chart\n",
    "if provider_names:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(provider_names)))\n",
    "    bars = ax.bar(provider_names, token_counts, color=colors)\n",
    "\n",
    "    ax.set_ylabel(\"Tokens Used\")\n",
    "    ax.set_title(\"Token Usage Comparison Across Providers\")\n",
    "    ax.set_xlabel(\"Provider\")\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, token_counts):\n",
    "        height = bar.get_height()\n",
    "        ax.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0, height, f\"{int(count)}\", ha=\"center\", va=\"bottom\"\n",
    "        )\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization. Please configure API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps <a id='next-steps'></a>\n",
    "\n",
    "Now that you've learned the basics, you can:\n",
    "\n",
    "1. **Run Benchmarks**: See the `02_benchmarking.ipynb` notebook\n",
    "2. **Custom Evaluations**: Check out `03_custom_evaluation.ipynb`\n",
    "3. **Fine-Tuning**: Explore `04_fine_tuning_workflow.ipynb`\n",
    "4. **Monitoring**: Learn about monitoring in `05_monitoring_setup.ipynb`\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- LLM Lab provides a unified interface for multiple LLM providers\n",
    "- Easy comparison of responses across different models\n",
    "- Built-in cost tracking and analysis\n",
    "- Extensible framework for custom evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "\n",
    "Don't forget to clean up resources if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up (if needed)\n",
    "print(\"Notebook completed successfully!\")\n",
    "print(f\"Total providers tested: {len(providers)}\")\n",
    "print(f\"Total prompts processed: {len(test_prompts) * len(providers)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
