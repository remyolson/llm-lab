{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking LLMs with LLM Lab\n",
    "\n",
    "This notebook demonstrates how to run comprehensive benchmarks across multiple LLM providers.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup](#setup)\n",
    "2. [Running Standard Benchmarks](#standard-benchmarks)\n",
    "3. [Custom Benchmark Creation](#custom-benchmarks)\n",
    "4. [Results Analysis](#results-analysis)\n",
    "5. [Performance Comparison](#performance-comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Setup path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import LLM Lab modules\n",
    "from src.benchmarks import BenchmarkRunner\n",
    "from src.evaluation.improved_evaluation import calculate_accuracy, calculate_f1_score\n",
    "from src.logging.results_logger import ResultsLogger\n",
    "from src.providers import AnthropicProvider, GoogleProvider, OpenAIProvider\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Standard Benchmarks <a id='standard-benchmarks'></a>\n",
    "\n",
    "LLM Lab includes several standard benchmarks:\n",
    "- **TruthfulQA**: Measures truthfulness and accuracy\n",
    "- **GSM8K**: Grade school math problems\n",
    "- **HellaSWAG**: Commonsense reasoning\n",
    "- **ARC**: AI2 Reasoning Challenge\n",
    "- **MMLU**: Massive Multitask Language Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize providers\n",
    "providers = []\n",
    "\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    providers.append(OpenAIProvider(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo\"))\n",
    "\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    providers.append(GoogleProvider(api_key=os.getenv(\"GOOGLE_API_KEY\"), model=\"gemini-1.5-flash\"))\n",
    "\n",
    "print(f\"Initialized {len(providers)} providers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a quick truthfulness benchmark\n",
    "if providers:\n",
    "    runner = BenchmarkRunner(providers[0])\n",
    "\n",
    "    # Run on a small subset for demonstration\n",
    "    results = runner.run_benchmark(\n",
    "        benchmark_name=\"truthfulness\",\n",
    "        num_samples=5,  # Small sample for demo\n",
    "    )\n",
    "\n",
    "    print(\"Benchmark Results:\")\n",
    "    print(f\"Accuracy: {results.get('accuracy', 0):.2%}\")\n",
    "    print(f\"Average response time: {results.get('avg_time', 0):.2f}s\")\n",
    "    print(f\"Total tokens used: {results.get('total_tokens', 0)}\")\n",
    "else:\n",
    "    print(\"No providers available. Please set API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Benchmark Creation <a id='custom-benchmarks'></a>\n",
    "\n",
    "Let's create a custom benchmark for domain-specific evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom benchmark questions\n",
    "custom_benchmark = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"expected_answer\": \"Paris\",\n",
    "        \"category\": \"geography\",\n",
    "    },\n",
    "    {\"question\": \"What is 15 + 27?\", \"expected_answer\": \"42\", \"category\": \"math\"},\n",
    "    {\n",
    "        \"question\": \"Who wrote 'Romeo and Juliet'?\",\n",
    "        \"expected_answer\": \"William Shakespeare\",\n",
    "        \"category\": \"literature\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the chemical symbol for water?\",\n",
    "        \"expected_answer\": \"H2O\",\n",
    "        \"category\": \"science\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"In what year did World War II end?\",\n",
    "        \"expected_answer\": \"1945\",\n",
    "        \"category\": \"history\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Created custom benchmark with {len(custom_benchmark)} questions\")\n",
    "print(f\"Categories: {set(q['category'] for q in custom_benchmark)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run custom benchmark\n",
    "def run_custom_benchmark(provider, questions):\n",
    "    results = []\n",
    "\n",
    "    for q in questions:\n",
    "        try:\n",
    "            response = provider.generate(\n",
    "                prompt=q[\"question\"],\n",
    "                max_tokens=50,\n",
    "                temperature=0.1,  # Low temperature for factual answers\n",
    "            )\n",
    "\n",
    "            answer = response.get(\"content\", \"\").strip()\n",
    "            is_correct = q[\"expected_answer\"].lower() in answer.lower()\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"question\": q[\"question\"],\n",
    "                    \"expected\": q[\"expected_answer\"],\n",
    "                    \"actual\": answer[:100],  # Truncate long answers\n",
    "                    \"correct\": is_correct,\n",
    "                    \"category\": q[\"category\"],\n",
    "                }\n",
    "            )\n",
    "        except Exception as e:\n",
    "            results.append(\n",
    "                {\n",
    "                    \"question\": q[\"question\"],\n",
    "                    \"expected\": q[\"expected_answer\"],\n",
    "                    \"actual\": f\"Error: {str(e)}\",\n",
    "                    \"correct\": False,\n",
    "                    \"category\": q[\"category\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run benchmark on available providers\n",
    "if providers:\n",
    "    benchmark_results = {}\n",
    "    for provider in providers:\n",
    "        print(f\"\\nRunning benchmark on {provider.provider_name}...\")\n",
    "        results = run_custom_benchmark(provider, custom_benchmark)\n",
    "        benchmark_results[provider.provider_name] = results\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = sum(r[\"correct\"] for r in results) / len(results)\n",
    "        print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "        # Show results by category\n",
    "        df = pd.DataFrame(results)\n",
    "        category_accuracy = df.groupby(\"category\")[\"correct\"].mean()\n",
    "        print(\"\\nAccuracy by category:\")\n",
    "        print(category_accuracy.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis <a id='results-analysis'></a>\n",
    "\n",
    "Let's analyze and visualize the benchmark results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed results DataFrame\n",
    "if benchmark_results:\n",
    "    all_results = []\n",
    "    for provider_name, results in benchmark_results.items():\n",
    "        for r in results:\n",
    "            all_results.append({\"provider\": provider_name, **r})\n",
    "\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    # Summary statistics\n",
    "    print(\"Overall Results Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    summary = df_results.groupby(\"provider\")[\"correct\"].agg([\"sum\", \"count\", \"mean\"])\n",
    "    summary.columns = [\"Correct\", \"Total\", \"Accuracy\"]\n",
    "    summary[\"Accuracy\"] = summary[\"Accuracy\"].apply(lambda x: f\"{x:.2%}\")\n",
    "    print(summary)\n",
    "\n",
    "    # Detailed category breakdown\n",
    "    print(\"\\nCategory Performance Matrix\")\n",
    "    print(\"=\" * 50)\n",
    "    pivot_table = pd.pivot_table(\n",
    "        df_results, values=\"correct\", index=\"category\", columns=\"provider\", aggfunc=\"mean\"\n",
    "    )\n",
    "    print(pivot_table.applymap(lambda x: f\"{x:.0%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison <a id='performance-comparison'></a>\n",
    "\n",
    "Let's create visualizations to compare provider performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "if benchmark_results and len(benchmark_results) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "    # Overall accuracy comparison\n",
    "    ax1 = axes[0]\n",
    "    provider_accuracy = df_results.groupby(\"provider\")[\"correct\"].mean()\n",
    "    provider_accuracy.plot(\n",
    "        kind=\"bar\", ax=ax1, color=sns.color_palette(\"husl\", len(provider_accuracy))\n",
    "    )\n",
    "    ax1.set_title(\"Overall Accuracy by Provider\", fontsize=14, fontweight=\"bold\")\n",
    "    ax1.set_xlabel(\"Provider\")\n",
    "    ax1.set_ylabel(\"Accuracy\")\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: \"{:.0%}\".format(y)))\n",
    "\n",
    "    # Category performance heatmap\n",
    "    ax2 = axes[1]\n",
    "    pivot_data = pd.pivot_table(\n",
    "        df_results, values=\"correct\", index=\"category\", columns=\"provider\", aggfunc=\"mean\"\n",
    "    )\n",
    "    sns.heatmap(\n",
    "        pivot_data, annot=True, fmt=\".0%\", cmap=\"YlOrRd\", ax=ax2, cbar_kws={\"label\": \"Accuracy\"}\n",
    "    )\n",
    "    ax2.set_title(\"Performance Heatmap by Category\", fontsize=14, fontweight=\"bold\")\n",
    "    ax2.set_xlabel(\"Provider\")\n",
    "    ax2.set_ylabel(\"Category\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Detailed results table\n",
    "    print(\"\\nSample Responses Comparison\")\n",
    "    print(\"=\" * 80)\n",
    "    for idx, row in df_results.head(3).iterrows():\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"Provider: {row['provider']}\")\n",
    "        print(f\"Expected: {row['expected']}\")\n",
    "        print(f\"Actual: {row['actual']}\")\n",
    "        print(f\"Correct: {'✓' if row['correct'] else '✗'}\")\n",
    "        print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Benchmarking Features\n",
    "\n",
    "### Batch Processing and Parallel Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of batch processing for efficiency\n",
    "def batch_benchmark(provider, questions, batch_size=5):\n",
    "    \"\"\"Run benchmark in batches for better efficiency.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(questions), batch_size):\n",
    "        batch = questions[i : i + batch_size]\n",
    "        batch_prompts = [q[\"question\"] for q in batch]\n",
    "\n",
    "        try:\n",
    "            # Use batch generation if available\n",
    "            if hasattr(provider, \"batch_generate\"):\n",
    "                responses = provider.batch_generate(batch_prompts, max_tokens=50)\n",
    "                for q, resp in zip(batch, responses):\n",
    "                    answer = resp.get(\"content\", \"\").strip()\n",
    "                    is_correct = q[\"expected_answer\"].lower() in answer.lower()\n",
    "                    results.append(\n",
    "                        {\"question\": q[\"question\"], \"correct\": is_correct, \"batch_processed\": True}\n",
    "                    )\n",
    "            else:\n",
    "                # Fall back to sequential processing\n",
    "                for q in batch:\n",
    "                    resp = provider.generate(q[\"question\"], max_tokens=50)\n",
    "                    answer = resp.get(\"content\", \"\").strip()\n",
    "                    is_correct = q[\"expected_answer\"].lower() in answer.lower()\n",
    "                    results.append(\n",
    "                        {\"question\": q[\"question\"], \"correct\": is_correct, \"batch_processed\": False}\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(f\"Batch processing error: {e}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Batch processing function defined for efficient benchmarking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results\n",
    "\n",
    "Save benchmark results for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "if \"df_results\" in locals() and not df_results.empty:\n",
    "    output_path = project_root / \"results\" / \"benchmark_results.csv\"\n",
    "    output_path.parent.mkdir(exist_ok=True)\n",
    "    df_results.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "    # Create summary report\n",
    "    summary_report = {\n",
    "        \"date\": pd.Timestamp.now().isoformat(),\n",
    "        \"providers_tested\": list(benchmark_results.keys()),\n",
    "        \"num_questions\": len(custom_benchmark),\n",
    "        \"overall_accuracy\": df_results.groupby(\"provider\")[\"correct\"].mean().to_dict(),\n",
    "        \"category_performance\": pivot_table.to_dict() if \"pivot_table\" in locals() else {},\n",
    "    }\n",
    "\n",
    "    import json\n",
    "\n",
    "    summary_path = project_root / \"results\" / \"benchmark_summary.json\"\n",
    "    with open(summary_path, \"w\") as f:\n",
    "        json.dump(summary_report, f, indent=2, default=str)\n",
    "    print(f\"Summary report saved to: {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. Running standard benchmarks with LLM Lab\n",
    "2. Creating custom benchmarks for domain-specific evaluation\n",
    "3. Analyzing and visualizing benchmark results\n",
    "4. Comparing performance across multiple providers\n",
    "5. Batch processing for efficient benchmarking\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore more advanced benchmarks (GSM8K, MMLU, etc.)\n",
    "- Create domain-specific evaluation metrics\n",
    "- Set up automated benchmarking pipelines\n",
    "- Integrate with monitoring for continuous evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
